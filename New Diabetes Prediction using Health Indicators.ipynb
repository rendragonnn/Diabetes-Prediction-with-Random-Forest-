{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd140ef",
   "metadata": {
    "papermill": {
     "duration": 0.005558,
     "end_time": "2025-05-28T18:45:17.827507",
     "exception": false,
     "start_time": "2025-05-28T18:45:17.821949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Diabetes Prediction Analysis - Processed Dataset\n",
    "\n",
    "This notebook implements a comprehensive machine learning analysis for diabetes prediction using the processed diabetes dataset (CDC BRFSS data). The analysis includes:\n",
    "\n",
    "1. Data Exploration and Visualization\n",
    "2. Data Preprocessing and Feature Engineering\n",
    "3. Multiple Machine Learning Algorithms\n",
    "4. Model Evaluation and Comparison\n",
    "5. Feature Importance Analysis\n",
    "6. Results Interpretation\n",
    "\n",
    "**Dataset**: The processed diabetes dataset contains health indicators and lifestyle factors from CDC's Behavioral Risk Factor Surveillance System (BRFSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c12c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:17.839326Z",
     "iopub.status.busy": "2025-05-28T18:45:17.838994Z",
     "iopub.status.idle": "2025-05-28T18:45:22.861027Z",
     "shell.execute_reply": "2025-05-28T18:45:22.859967Z"
    },
    "papermill": {
     "duration": 5.029726,
     "end_time": "2025-05-28T18:45:22.862563",
     "exception": false,
     "start_time": "2025-05-28T18:45:17.832837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d083f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:22.874042Z",
     "iopub.status.busy": "2025-05-28T18:45:22.873531Z",
     "iopub.status.idle": "2025-05-28T18:45:23.723628Z",
     "shell.execute_reply": "2025-05-28T18:45:23.722420Z"
    },
    "papermill": {
     "duration": 0.858046,
     "end_time": "2025-05-28T18:45:23.725489",
     "exception": false,
     "start_time": "2025-05-28T18:45:22.867443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_diabetes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a73011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:23.736741Z",
     "iopub.status.busy": "2025-05-28T18:45:23.736389Z",
     "iopub.status.idle": "2025-05-28T18:45:24.108101Z",
     "shell.execute_reply": "2025-05-28T18:45:24.106843Z"
    },
    "papermill": {
     "duration": 0.379091,
     "end_time": "2025-05-28T18:45:24.109636",
     "exception": false,
     "start_time": "2025-05-28T18:45:23.730545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['BMI_Category', 'Age_Group', 'GenHlth', 'Sex', 'Education', 'Income']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].unique()[:10]}...\")  # Show first 10 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ee0ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:24.121279Z",
     "iopub.status.busy": "2025-05-28T18:45:24.120979Z",
     "iopub.status.idle": "2025-05-28T18:45:27.346592Z",
     "shell.execute_reply": "2025-05-28T18:45:27.345470Z"
    },
    "papermill": {
     "duration": 3.236146,
     "end_time": "2025-05-28T18:45:27.351131",
     "exception": false,
     "start_time": "2025-05-28T18:45:24.114985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fdb37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:27.372732Z",
     "iopub.status.busy": "2025-05-28T18:45:27.372364Z",
     "iopub.status.idle": "2025-05-28T18:45:28.673604Z",
     "shell.execute_reply": "2025-05-28T18:45:28.672647Z"
    },
    "papermill": {
     "duration": 1.314731,
     "end_time": "2025-05-28T18:45:28.676250",
     "exception": false,
     "start_time": "2025-05-28T18:45:27.361519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explore key health indicators\n",
    "health_indicators = ['HighBP', 'HighChol', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', \n",
    "                    'Smoker', 'HvyAlcoholConsump', 'GenHlth', 'MentHlth', 'PhysHlth']\n",
    "\n",
    "# Filter indicators that exist in the dataset\n",
    "available_indicators = [col for col in health_indicators if col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686dd22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:28.702776Z",
     "iopub.status.busy": "2025-05-28T18:45:28.702384Z",
     "iopub.status.idle": "2025-05-28T18:45:28.814975Z",
     "shell.execute_reply": "2025-05-28T18:45:28.813650Z"
    },
    "papermill": {
     "duration": 0.127842,
     "end_time": "2025-05-28T18:45:28.816588",
     "exception": false,
     "start_time": "2025-05-28T18:45:28.688746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "categorical_mappings = {}\n",
    "if 'BMI_Category' in df_processed.columns and df_processed['BMI_Category'].dtype == 'object':\n",
    "    bmi_mapping = {'Underweight': 0, 'Normal': 1, 'Overweight': 2, 'Obese': 3}\n",
    "    df_processed['BMI_Category_encoded'] = df_processed['BMI_Category'].map(bmi_mapping)\n",
    "    categorical_mappings['BMI_Category'] = bmi_mapping\n",
    "\n",
    "if 'Age_Group' in df_processed.columns and df_processed['Age_Group'].dtype == 'object':\n",
    "    age_mapping = {'18-24': 0, '25-34': 1, '35-54': 2, '55-64': 3, '65+': 4}\n",
    "    df_processed['Age_Group_encoded'] = df_processed['Age_Group'].map(age_mapping)\n",
    "    categorical_mappings['Age_Group'] = age_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81dc6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:28.843580Z",
     "iopub.status.busy": "2025-05-28T18:45:28.843286Z",
     "iopub.status.idle": "2025-05-28T18:45:28.948155Z",
     "shell.execute_reply": "2025-05-28T18:45:28.946896Z"
    },
    "papermill": {
     "duration": 0.119927,
     "end_time": "2025-05-28T18:45:28.949806",
     "exception": false,
     "start_time": "2025-05-28T18:45:28.829879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Create new composite features\n",
    "if 'MentHlth' in df_processed.columns and 'PhysHlth' in df_processed.columns:\n",
    "    df_processed['Overall_Health_Score'] = df_processed['MentHlth'] + df_processed['PhysHlth']\n",
    "\n",
    "if 'HighBP' in df_processed.columns and 'HeartDiseaseorAttack' in df_processed.columns:\n",
    "    df_processed['Health_Issue_Both'] = (df_processed['HighBP'] == 1) & (df_processed['HeartDiseaseorAttack'] == 1)\n",
    "    df_processed['Health_Issue_Both'] = df_processed['Health_Issue_Both'].astype(int)\n",
    "\n",
    "# Count of health conditions\n",
    "health_conditions = ['HighBP', 'HighChol', 'Stroke', 'HeartDiseaseorAttack']\n",
    "available_conditions = [col for col in health_conditions if col in df_processed.columns]\n",
    "if len(available_conditions) > 1:\n",
    "    df_processed['Multiple_Conditions'] = df_processed[available_conditions].sum(axis=1)\n",
    "\n",
    "# Lifestyle score\n",
    "lifestyle_factors = ['PhysActivity', 'Fruits', 'Veggies']\n",
    "available_lifestyle = [col for col in lifestyle_factors if col in df_processed.columns]\n",
    "if len(available_lifestyle) > 1:\n",
    "    df_processed['Healthy_Lifestyle_Score'] = df_processed[available_lifestyle].sum(axis=1)\n",
    "\n",
    "# Risk factors score (negative health indicators)\n",
    "risk_factors = ['Smoker', 'HvyAlcoholConsump']\n",
    "available_risks = [col for col in risk_factors if col in df_processed.columns]\n",
    "if len(available_risks) > 1:\n",
    "    df_processed['Risk_Factors_Score'] = df_processed[available_risks].sum(axis=1)\n",
    "\n",
    "\n",
    "new_features = ['Overall_Health_Score', 'Health_Issue_Both', 'Multiple_Conditions', \n",
    "                'Healthy_Lifestyle_Score', 'Risk_Factors_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fced154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:28.976766Z",
     "iopub.status.busy": "2025-05-28T18:45:28.976392Z",
     "iopub.status.idle": "2025-05-28T18:45:29.259537Z",
     "shell.execute_reply": "2025-05-28T18:45:29.258413Z"
    },
    "papermill": {
     "duration": 0.298685,
     "end_time": "2025-05-28T18:45:29.261219",
     "exception": false,
     "start_time": "2025-05-28T18:45:28.962534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare features for machine learning\n",
    "# Identify numerical features (excluding the target variable)\n",
    "numerical_features = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Diabetes_binary' in numerical_features:\n",
    "    numerical_features.remove('Diabetes_binary')\n",
    "\n",
    "# Remove any string categorical columns that weren't encoded\n",
    "string_columns = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Final feature list for ML\n",
    "ml_features = [col for col in numerical_features if col not in string_columns]\n",
    "\n",
    "print(f\"\\nFeatures selected for ML ({len(ml_features)}):\")\n",
    "for i, feature in enumerate(ml_features):\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_processed[ml_features]\n",
    "y = df_processed['Diabetes_binary']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711b0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:29.288835Z",
     "iopub.status.busy": "2025-05-28T18:45:29.288495Z",
     "iopub.status.idle": "2025-05-28T18:45:29.779441Z",
     "shell.execute_reply": "2025-05-28T18:45:29.778382Z"
    },
    "papermill": {
     "duration": 0.506748,
     "end_time": "2025-05-28T18:45:29.781008",
     "exception": false,
     "start_time": "2025-05-28T18:45:29.274260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Standard Scaler\n",
    "scaler_std = StandardScaler()\n",
    "X_train_std = scaler_std.fit_transform(X_train)\n",
    "X_test_std = scaler_std.transform(X_test)\n",
    "\n",
    "# MinMax Scaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_test_minmax = scaler_minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1806b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:29.808170Z",
     "iopub.status.busy": "2025-05-28T18:45:29.807301Z",
     "iopub.status.idle": "2025-05-28T18:45:29.814704Z",
     "shell.execute_reply": "2025-05-28T18:45:29.813714Z"
    },
    "papermill": {
     "duration": 0.02227,
     "end_time": "2025-05-28T18:45:29.816011",
     "exception": false,
     "start_time": "2025-05-28T18:45:29.793741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591a39d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:45:29.843826Z",
     "iopub.status.busy": "2025-05-28T18:45:29.843469Z",
     "iopub.status.idle": "2025-05-28T18:53:03.547247Z",
     "shell.execute_reply": "2025-05-28T18:53:03.546268Z"
    },
    "papermill": {
     "duration": 453.731664,
     "end_time": "2025-05-28T18:53:03.561043",
     "exception": false,
     "start_time": "2025-05-28T18:45:29.829379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate Models with Pipeline Approach\n",
    "\n",
    "results = []\n",
    "model_performance = {}\n",
    "\n",
    "# Create pipelines for each model that handle missing values\n",
    "model_pipelines = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create pipeline with imputer + scaler + model\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
    "        ('scaler', StandardScaler()),                 # Scale features\n",
    "        ('classifier', model)                         # The actual model\n",
    "    ])\n",
    "    model_pipelines[name] = pipeline\n",
    "\n",
    "# Train and evaluate each pipeline\n",
    "for name, pipeline in model_pipelines.items():\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    if hasattr(pipeline.named_steps['classifier'], 'predict_proba'):\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'CV_Mean': cv_scores.mean(),\n",
    "        'CV_Std': cv_scores.std(),\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    model_performance[name] = {\n",
    "        'model': pipeline,  # Store the entire pipeline\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cdb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:53:03.588816Z",
     "iopub.status.busy": "2025-05-28T18:53:03.588451Z",
     "iopub.status.idle": "2025-05-28T18:53:03.602383Z",
     "shell.execute_reply": "2025-05-28T18:53:03.601316Z"
    },
    "papermill": {
     "duration": 0.029673,
     "end_time": "2025-05-28T18:53:03.603981",
     "exception": false,
     "start_time": "2025-05-28T18:53:03.574308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create results DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Find best performing model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_accuracy = results_df.iloc[0]['Accuracy']\n",
    "best_f1 = results_df.iloc[0]['F1_Score']\n",
    "\n",
    "# Display top 3 models\n",
    "for i, (_, row) in enumerate(results_df.head(3).iterrows()):\n",
    "    print(f\"{i+1}. {row['Model']}: Accuracy={row['Accuracy']:.4f}, F1={row['F1_Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02765b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:53:04.487251Z",
     "iopub.status.busy": "2025-05-28T18:53:04.486400Z",
     "iopub.status.idle": "2025-05-28T18:53:04.993706Z",
     "shell.execute_reply": "2025-05-28T18:53:04.992837Z"
    },
    "papermill": {
     "duration": 0.525911,
     "end_time": "2025-05-28T18:53:04.995765",
     "exception": false,
     "start_time": "2025-05-28T18:53:04.469854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detailed Analysis of Best Model\n",
    "best_model = model_performance[best_model_name]['model']\n",
    "best_predictions = model_performance[best_model_name]['predictions']\n",
    "best_probabilities = model_performance[best_model_name]['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e7e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:53:05.033227Z",
     "iopub.status.busy": "2025-05-28T18:53:05.032928Z",
     "iopub.status.idle": "2025-05-28T18:53:05.339135Z",
     "shell.execute_reply": "2025-05-28T18:53:05.338049Z"
    },
    "papermill": {
     "duration": 0.327465,
     "end_time": "2025-05-28T18:53:05.341522",
     "exception": false,
     "start_time": "2025-05-28T18:53:05.014057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "# Get the actual model from the pipeline\n",
    "best_pipeline = model_performance[best_model_name]['model']\n",
    "best_model = best_pipeline.named_steps['classifier']\n",
    "\n",
    "# Get feature importance (for tree-based models) or coefficients (for linear models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based models\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    importance_type = \"Feature Importance\"\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Linear models\n",
    "    feature_importance = np.abs(best_model.coef_[0])\n",
    "    importance_type = \"Coefficient Magnitude\"\n",
    "else:\n",
    "    # For models without feature importance, use permutation importance\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    perm_importance = permutation_importance(best_pipeline, X_test, y_test, random_state=42)\n",
    "    feature_importance = perm_importance.importances_mean\n",
    "    importance_type = \"Permutation Importance\"\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'Feature': ml_features,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "\n",
    "for i, (_, row) in enumerate(feature_imp_df.head(10).iterrows()):\n",
    "    print(f\"{i+1:2d}. {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ababdf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:53:05.381897Z",
     "iopub.status.busy": "2025-05-28T18:53:05.381557Z",
     "iopub.status.idle": "2025-05-28T19:15:32.406793Z",
     "shell.execute_reply": "2025-05-28T19:15:32.405509Z"
    },
    "papermill": {
     "duration": 1347.048468,
     "end_time": "2025-05-28T19:15:32.410326",
     "exception": false,
     "start_time": "2025-05-28T18:53:05.361858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Best Model\n",
    "\n",
    "# Define parameter grids for different models (with pipeline prefixes)\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [5, 10, 15, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    # Create a fresh pipeline\n",
    "    fresh_model = models[best_model_name]\n",
    "    if 'random_state' in fresh_model.get_params():\n",
    "        fresh_model.set_params(random_state=42)\n",
    "    \n",
    "    fresh_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', fresh_model)\n",
    "    ])\n",
    "    \n",
    "    # Grid search with cross-validation (use smaller CV for large dataset)\n",
    "    cv_folds = 3 if len(X_train) > 50000 else 5\n",
    "    grid_search = GridSearchCV(\n",
    "        fresh_pipeline, \n",
    "        param_grids[best_model_name], \n",
    "        cv=cv_folds, \n",
    "        scoring='f1', \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    tuned_predictions = tuned_model.predict(X_test)\n",
    "    tuned_accuracy = accuracy_score(y_test, tuned_predictions)\n",
    "    tuned_f1 = f1_score(y_test, tuned_predictions)\n",
    "    \n",
    "    improvement_acc = tuned_accuracy - best_accuracy\n",
    "    improvement_f1 = tuned_f1 - best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b7e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T19:15:32.452460Z",
     "iopub.status.busy": "2025-05-28T19:15:32.451350Z",
     "iopub.status.idle": "2025-05-28T19:25:11.407607Z",
     "shell.execute_reply": "2025-05-28T19:25:11.406716Z"
    },
    "papermill": {
     "duration": 578.979519,
     "end_time": "2025-05-28T19:25:11.410030",
     "exception": false,
     "start_time": "2025-05-28T19:15:32.430511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross-Validation Analysis\n",
    "\n",
    "# Perform detailed cross-validation for top 3 models\n",
    "top_models = results_df.head(3)['Model'].tolist()\n",
    "\n",
    "cv_results = []\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Use 5-fold for large dataset\n",
    "\n",
    "for model_name in top_models:\n",
    "    \n",
    "    # Get the pipeline for this model\n",
    "    pipeline = model_pipelines[model_name]\n",
    "    \n",
    "    # 5-fold cross-validation with multiple metrics\n",
    "    cv_accuracy = cross_val_score(pipeline, X_train, y_train, cv=skfold, scoring='accuracy')\n",
    "    cv_precision = cross_val_score(pipeline, X_train, y_train, cv=skfold, scoring='precision')\n",
    "    cv_recall = cross_val_score(pipeline, X_train, y_train, cv=skfold, scoring='recall')\n",
    "    cv_f1 = cross_val_score(pipeline, X_train, y_train, cv=skfold, scoring='f1')\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy_Mean': cv_accuracy.mean(),\n",
    "        'Accuracy_Std': cv_accuracy.std(),\n",
    "        'Precision_Mean': cv_precision.mean(),\n",
    "        'Precision_Std': cv_precision.std(),\n",
    "        'Recall_Mean': cv_recall.mean(),\n",
    "        'Recall_Std': cv_recall.std(),\n",
    "        'F1_Mean': cv_f1.mean(),\n",
    "        'F1_Std': cv_f1.std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7487422,
     "sourceId": 11910033,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2401.439661,
   "end_time": "2025-05-28T19:25:14.234641",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T18:45:12.794980",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
